{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Nome:**\n",
        "\n",
        "**RA:**\n",
        "\n",
        "Transforme este notebook num relatório técnico de experimentos com uma arquitetura VAE (Variational Autoencoder).\n",
        "\n",
        "Adicione comentários aos diferentes blocos de código, explicitando o propósito de cada bloco.\n",
        "\n",
        "Como é construído o espaço latente? Mostre resultados diferentes para diferentes configurações.\n",
        "\n",
        "Altere parâmetros de treinamento do modelo.\n",
        "\n",
        "Proponha maneiras de avaliar a qualidade das amostras geradas.\n",
        "\n",
        "### Attr2Font\n",
        "\n",
        "* Github: https://github.com/hologerry/Attr2Font\n",
        "* Dataset: https://drive.google.com/file/d/1TTqAklfsAp6KOPxCVl2jktH8kN4lEmI_/view"
      ],
      "metadata": {
        "id": "vVjzdW_u9255"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pacotes"
      ],
      "metadata": {
        "id": "Vmz_M3gT-KWG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6RRl7S3oAGc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets,transforms\n",
        "from torchvision.utils import save_image\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "random.seed(5)\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='svg'\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "import numpy as np\n",
        "# Dataset Attr2font"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Drive"
      ],
      "metadata": {
        "id": "uVhL1VN9-Ly0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CURtytpyoV_C"
      },
      "outputs": [],
      "source": [
        "## Conexão com o google drive, se achar necessário\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dados:"
      ],
      "metadata": {
        "id": "xxyZXUbd-Or-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixando os dados do Attr2font que podem ser encontrados em:\n",
        "!gdown --id 1TTqAklfsAp6KOPxCVl2jktH8kN4lEmI_\n",
        "!unzip ../content/explor_all.zip"
      ],
      "metadata": {
        "id": "KIoJ7Ko85fZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria os diretórios de interesse:\n",
        "!mkdir data\n",
        "!mkdir results\n",
        "!mkdir models\n",
        "!mv ../content/explor_all/image ../content/data/Fonts\n",
        "!rm -rf ../content/explor_all"
      ],
      "metadata": {
        "id": "ifx0T6W290u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F16nVre5o9yL"
      },
      "outputs": [],
      "source": [
        "epochs=5\n",
        "batch_size=64\n",
        "torch.manual_seed(17)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p6W0oyxqvzI"
      },
      "outputs": [],
      "source": [
        "def show_img(img):\n",
        "    img = img.permute(1, 2, 0)\n",
        "    if img.shape[2]==1:\n",
        "        img = img.view(img.shape[0], img.shape[1])\n",
        "    plt.title(f'Image has size {img.cpu().numpy().shape}')\n",
        "    plt.imshow(img,cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path='../content/data/Fonts'\n",
        "train_path = '../content/data/trainFonts'\n",
        "val_path = '../content/data/valFonts'"
      ],
      "metadata": {
        "id": "8R1gW_Di6pQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3MeGWtDq0jC"
      },
      "outputs": [],
      "source": [
        "transforms_all = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize(size=50),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dummy_batch = torch.utils.data.DataLoader(\n",
        "    datasets.ImageFolder(path,transform=transforms.ToTensor()),\n",
        "    batch_size=1, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uybPom9dlXZv"
      },
      "outputs": [],
      "source": [
        "for batch in dummy_batch:\n",
        "  original_image=batch[0][0]\n",
        "  show_img(original_image)\n",
        "  show_img(transforms_all(transforms.ToPILImage()(original_image)))\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separação em dados de treino e validação"
      ],
      "metadata": {
        "id": "3MnnkzIA-jhy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEMydNjynWnB"
      },
      "outputs": [],
      "source": [
        "folders=os.listdir(path)\n",
        "print(len(folders))\n",
        "\n",
        "# shutil.rmtree('/content/drive/MyDrive/Corpora/trainFonts')\n",
        "# shutil.rmtree('/content/drive/MyDrive/Corpora/valFonts')\n",
        "\n",
        "os.mkdir(train_path) #'../content/data/trainFonts'\n",
        "os.mkdir(val_path)   #'../content/data/valFonts'\n",
        "\n",
        "\n",
        "# Podemos limitar em num_folders a quantidade total de pasta que iremos trabalhar, no total temos 1116.\n",
        "num_folders = 134\n",
        "count_folders = 1\n",
        "for folder in tqdm(folders):\n",
        "    if count_folders <= num_folders:\n",
        "      print(folder)\n",
        "      os.mkdir(f'{train_path}/{folder}')\n",
        "      os.mkdir(f'{val_path}/{folder}')\n",
        "      images = os.listdir(f'{path}/{folder}')\n",
        "      random.shuffle(images)\n",
        "      for image in images[:20]:\n",
        "        shutil.copy(f'{path}/{folder}/{image}',\n",
        "                  f'{val_path}/{folder}/{image}')\n",
        "      for image in images[20:]:\n",
        "        shutil.copy(f'{path}/{folder}/{image}',\n",
        "                  f'{train_path}/{folder}/{image}')\n",
        "    count_folders += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as0w1nHXAWJr"
      },
      "outputs": [],
      "source": [
        "print(len(os.listdir(train_path)))\n",
        "print(len(os.listdir(val_path)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construção do Dataset e Dataloader"
      ],
      "metadata": {
        "id": "xXeqECWk-sb0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps0SGu7kCRU-"
      },
      "outputs": [],
      "source": [
        "transforms_set = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize(size=50),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_loader=torch.utils.data.DataLoader(\n",
        "    datasets.ImageFolder(f'{train_path}',transform=transforms_set),\n",
        "    batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "val_loader=torch.utils.data.DataLoader(\n",
        "    datasets.ImageFolder(f'{val_path}',transform=transforms_set),\n",
        "    batch_size=batch_size, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementação VAE"
      ],
      "metadata": {
        "id": "V0WnhejB-xeY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMucNt88G83a"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "\n",
        "        z = 32 # Dimensão do espaço latente\n",
        "\n",
        "        #ENCODER\n",
        "        self.fc1 = nn.Linear(50*50,1000) # Inclua comentários\n",
        "        self.fc21 = nn.Linear(1000,z)   # Inclua comentários\n",
        "        self.fc22 = nn.Linear(1000,z)   # Inclua comentários\n",
        "\n",
        "        #DECODER\n",
        "        self.fc3 = nn.Linear(z,1000)\n",
        "        self.fc4 = nn.Linear(1000,50*50)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # don't forget forward pass re-index\n",
        "\n",
        "        mu, logvar = self.encode(x.view(-1, 50*50))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementação Loss Function"
      ],
      "metadata": {
        "id": "BMCnykAu-zPu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tF7NcN_gINjd"
      },
      "outputs": [],
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 50*50), reduction='sum')\n",
        "\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "    return BCE + KLD"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criação do Modelo"
      ],
      "metadata": {
        "id": "jTNbip80-6uz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvBcXxXhI7ME"
      },
      "outputs": [],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model=VAE().to(device)\n",
        "print('Device: ',device)\n",
        "model=VAE().to(device)\n",
        "optimizer=optim.Adam(model.parameters(),lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loops de Treinamento e Avaliação"
      ],
      "metadata": {
        "id": "4QPRx-BE--rV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm-ofYHEN6MP"
      },
      "outputs": [],
      "source": [
        "def evaluate(evaluate_data=val_loader):\n",
        "\n",
        "\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(evaluate_data):\n",
        "\n",
        "            data=data.to(device)\n",
        "            recon_batch,mu,logvar=model(data)\n",
        "            val_loss += loss_function(recon_batch,data,mu,logvar).item()\n",
        "\n",
        "\n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 16)\n",
        "                comparison = torch.cat([data[:n],\n",
        "                                        recon_batch.view(batch_size, 1, 50, 50)[:n]])\n",
        "                save_image(comparison.cpu(),\n",
        "                           '../content/results/reconstruction_' + str(epoch) + '.pdf', nrow=n)\n",
        "\n",
        "    val_loss /= len(evaluate_data.dataset)\n",
        "    return val_loss\n",
        "\n",
        "\n",
        "def sample_latent_space(epoch):\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # how to sample from our latent space\n",
        "        sample=torch.randn(64,32).to(device)\n",
        "        sample=model.decode(sample).cpu()\n",
        "\n",
        "        save_image(sample.view(64, 1, 50, 50),\n",
        "                   '../content/results/sample_' + str(epoch) + '.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOWtfLfqPmC-"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc='Epoch {:03d}'.format(epoch), leave=False, disable=False)\n",
        "    for data, _ in progress_bar:\n",
        "\n",
        "        data=data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch,mu,logvar=model(data)\n",
        "\n",
        "        loss=loss_function(recon_batch,data,mu,logvar)\n",
        "        loss.backward()\n",
        "        train_loss+=loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(data))})\n",
        "\n",
        "    average_train_loss = train_loss / len(train_loader.dataset)\n",
        "    tqdm.write('Training set loss (average, epoch {:03d}): {:.3f}'.format(epoch, average_train_loss))\n",
        "    val_loss = evaluate(val_loader)\n",
        "    tqdm.write('\\t\\t\\t\\t====> Validation set loss: {:.3f}'.format(val_loss))\n",
        "\n",
        "    train_losses.append(average_train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if epoch%300==0:\n",
        "        torch.save(model.state_dict(), f'../content/models/epoch_{epoch}.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinamento"
      ],
      "metadata": {
        "id": "FvVgV4VG_Bce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4geSKYYQcLJ"
      },
      "outputs": [],
      "source": [
        "epochs = 300\n",
        "\n",
        "train_losses,val_losses=[],[]\n",
        "\n",
        "for epoch in range(1,epochs+1):\n",
        "  train(epoch)\n",
        "  sample_latent_space(epoch)\n",
        "\n",
        "np.savetxt('../content/models/training_losses.txt', np.array(train_losses), delimiter='\\n')\n",
        "np.savetxt('../content/models/validation_losses.txt', np.array(val_losses), delimiter='\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMK-UdW8WO8I"
      },
      "outputs": [],
      "source": [
        "train_losses=np.loadtxt('../content/models/training_losses.txt')\n",
        "val_losses=np.loadtxt('../content/models/validation_losses.txt')\n",
        "plt.plot(\n",
        "    range(1,len(train_losses)+1),\n",
        "    train_losses,\n",
        "    label='Training Losses',\n",
        "    linewidth=2,\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.plot(\n",
        "    range(1,len(val_losses)+1),\n",
        "    val_losses,\n",
        "    label='Validation Losses',\n",
        "    linewidth=2,\n",
        "    alpha=0.7\n",
        ")\n",
        "\n",
        "plt.title('VAE Font Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0kKnphJYFKs"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(\n",
        "    torch.load('../content/models/epoch_900.model',\n",
        "               map_location=torch.device('cpu')))\n",
        "\n",
        "sample_latent_space('a')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Zipa os resultados para download\n",
        "!zip -r results.zip ../content/results\n",
        "!zip -r models.zip ../content/models"
      ],
      "metadata": {
        "id": "pJAKjz7vlI_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixa arquivos para o pc local\n",
        "from google.colab import files\n",
        "files.download('/content/models.zip')\n",
        "files.download('/content/results.zip')"
      ],
      "metadata": {
        "id": "1bJTc4Jsnyx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_BZZoxsCvG34"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}